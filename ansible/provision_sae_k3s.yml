---
<<<<<<< HEAD
- name: Provision VMs on Proxmox and build runtime inventory
  hosts: localhost
  gather_facts: false
  vars_prompt:
    - name: proxmox_api_user
      prompt: "Proxmox API user (ex: root@pam)"
      private: false
    - name: proxmox_api_token_id
      prompt: "Proxmox token id (ex: ansible)"
      private: false
    - name: proxmox_api_token_secret
      prompt: "Proxmox token secret"
      private: true
    - name: template_vmid
      prompt: "Proxmox template VMID (recommended, leave empty to use name)"
      private: false
      default: ""
    - name: template_name
      prompt: "Proxmox template name (used if VMID is empty)"
      private: false
      default: ""
  vars:
    proxmox_api_host: "10.129.4.40"
    proxmox_api_port: "8006"
    proxmox_node: "pve"
    vm_gateway: "10.129.15.254"
    vm_netmask_cidr: "20"
    force_recreate_vms: false
    vm_disk_target_size: "20G"
    vm_ssh_user: "debian"
    vm_passwords:
      master: "master123"
      worker1: "worker123"
      worker2: "worker223"

    k3s_cluster_vms:
      - { name: "VM-Master", vmid: 301, ip: "10.129.5.203", cores: 2, memory: 4096 }
      - { name: "worker1", vmid: 302, ip: "10.129.5.204", cores: 2, memory: 3072 }
      - { name: "worker2", vmid: 303, ip: "10.129.5.205", cores: 2, memory: 3072 }

  pre_tasks:
    - name: Validate Proxmox API credentials are set
      ansible.builtin.assert:
        that:
          - proxmox_api_user | length > 0
          - proxmox_api_token_id | length > 0
          - proxmox_api_token_secret | length > 0
        fail_msg: "Set PROXMOX_API_USER, PROXMOX_API_TOKEN_ID and PROXMOX_API_TOKEN_SECRET before running this playbook."

    - name: Validate Proxmox credential format
      ansible.builtin.assert:
        that:
          - proxmox_api_user is match('^[^\s]+@[^\s]+$')
          - proxmox_api_token_id is match('^[A-Za-z0-9._-]+$')
          - proxmox_api_token_secret is not search('\s')
        fail_msg: "Invalid Proxmox credential format. Expected user like root@pam and token_id like ansible (no spaces, no URL)."

    - name: Validate template selector inputs
      ansible.builtin.assert:
        that:
          - (template_name | length > 0) or (template_vmid | length > 0)
        fail_msg: "Set PROXMOX_TEMPLATE_NAME or PROXMOX_TEMPLATE_VMID before running the playbook."

    - name: Verify Proxmox API token authentication
      ansible.builtin.uri:
        url: "https://{{ proxmox_api_host }}:{{ proxmox_api_port }}/api2/json/version"
        method: GET
        validate_certs: false
        headers:
          Authorization: "PVEAPIToken={{ proxmox_api_user }}!{{ proxmox_api_token_id }}={{ proxmox_api_token_secret }}"
        status_code: 200
        return_content: true
      register: proxmox_auth_check
      no_log: true

    - name: Confirm Proxmox auth precheck succeeded
      ansible.builtin.debug:
        msg: "Proxmox authentication precheck succeeded for {{ proxmox_api_user }}!{{ proxmox_api_token_id }}"

    - name: Fetch Proxmox VM resources
      ansible.builtin.uri:
        url: "https://{{ proxmox_api_host }}:{{ proxmox_api_port }}/api2/json/cluster/resources?type=vm"
        method: GET
        validate_certs: false
        headers:
          Authorization: "PVEAPIToken={{ proxmox_api_user }}!{{ proxmox_api_token_id }}={{ proxmox_api_token_secret }}"
        status_code: 200
        return_content: true
      register: proxmox_vm_resources
      no_log: true

    - name: Build template candidate lists
      ansible.builtin.set_fact:
        proxmox_template_names: "{{ proxmox_vm_resources.json.data | selectattr('template', 'defined') | selectattr('template', 'equalto', 1) | map(attribute='name') | list }}"
        proxmox_template_vmids: "{{ proxmox_vm_resources.json.data | selectattr('template', 'defined') | selectattr('template', 'equalto', 1) | map(attribute='vmid') | map('string') | list }}"
        proxmox_template_map: "{{ dict((proxmox_vm_resources.json.data | selectattr('template', 'defined') | selectattr('template', 'equalto', 1) | map(attribute='vmid') | map('string') | list) | zip(proxmox_vm_resources.json.data | selectattr('template', 'defined') | selectattr('template', 'equalto', 1) | map(attribute='name') | list)) }}"

    - name: Validate template exists in Proxmox
      ansible.builtin.assert:
        that:
          - (template_vmid | length == 0) or (template_vmid in proxmox_template_vmids)
          - (template_name | length == 0) or (template_name in proxmox_template_names)
        fail_msg: "Template not found. Available template names={{ proxmox_template_names }} vmids={{ proxmox_template_vmids }}"

    - name: Select clone source name
      ansible.builtin.set_fact:
        clone_source: "{{ proxmox_template_map[template_vmid] if (template_vmid | length > 0) else template_name }}"

    - name: Resolve selected template VMID
      ansible.builtin.set_fact:
        selected_template_vmid: "{{ template_vmid if (template_vmid | length > 0) else (proxmox_vm_resources.json.data | selectattr('name','equalto',template_name) | map(attribute='vmid') | map('string') | first) }}"

    - name: Read selected template config
      ansible.builtin.uri:
        url: "https://{{ proxmox_api_host }}:{{ proxmox_api_port }}/api2/json/nodes/{{ proxmox_node }}/qemu/{{ selected_template_vmid }}/config"
        method: GET
        validate_certs: false
        headers:
          Authorization: "PVEAPIToken={{ proxmox_api_user }}!{{ proxmox_api_token_id }}={{ proxmox_api_token_secret }}"
        status_code: 200
      register: selected_template_config
      no_log: true

    - name: Resolve available template system disk key
      ansible.builtin.set_fact:
        template_system_disk_key: >-
          {% if selected_template_config.json.data.scsi0 is defined %}scsi0
          {% elif selected_template_config.json.data.ide0 is defined %}ide0
          {% elif selected_template_config.json.data.virtio0 is defined %}virtio0
          {% elif selected_template_config.json.data.sata0 is defined %}sata0
          {% else %}none{% endif %}

    - name: Repair template bootdisk setting when inconsistent
      ansible.builtin.uri:
        url: "https://{{ proxmox_api_host }}:{{ proxmox_api_port }}/api2/json/nodes/{{ proxmox_node }}/qemu/{{ selected_template_vmid }}/config"
        method: PUT
        validate_certs: false
        headers:
          Authorization: "PVEAPIToken={{ proxmox_api_user }}!{{ proxmox_api_token_id }}={{ proxmox_api_token_secret }}"
        body_format: form-urlencoded
        body:
          bootdisk: "{{ template_system_disk_key }}"
          boot: "c"
        status_code: 200
      register: template_bootdisk_repair
      when:
        - template_system_disk_key != 'none'
        - selected_template_config.json.data.bootdisk is not defined or selected_template_config.json.data[selected_template_config.json.data.bootdisk] is not defined
      no_log: true
      failed_when: false

    - name: Validate selected template has an attached system disk
      ansible.builtin.assert:
        that:
          - template_system_disk_key != 'none'
        fail_msg: "Selected template is invalid (no system disk attached). Fix template {{ clone_source }} (VMID {{ selected_template_vmid }}) before running automation."

  tasks:
    - name: Remove stale SSH host keys for VM IPs
      ansible.builtin.command: "ssh-keygen -R {{ item.ip }}"
      loop: "{{ k3s_cluster_vms }}"
      loop_control:
        label: "{{ item.name }}"
      delegate_to: localhost
      become: false
      changed_when: false
      failed_when: false

    - name: Recreate VMs from clean state when requested
      community.proxmox.proxmox_kvm:
        api_host: "{{ proxmox_api_host }}"
        api_user: "{{ proxmox_api_user }}"
        api_token_id: "{{ proxmox_api_token_id }}"
        api_token_secret: "{{ proxmox_api_token_secret }}"
        validate_certs: false
        node: "{{ proxmox_node }}"
        vmid: "{{ item.vmid }}"
        state: absent
      loop: "{{ k3s_cluster_vms }}"
      loop_control:
        label: "{{ item.name }}"
      when: force_recreate_vms | bool
      failed_when: false

    - name: Read local public SSH key
=======
- name: Provision local Parallels VMs and build runtime inventory
  hosts: localhost
  gather_facts: false
  vars:
    parallels_template_vm: ""
    parallels_network_mode: "shared"
    force_recreate_vms: false

    vm_ssh_user: "debian"
    vm_ssh_password: "master123"

    local_app_image: "sae-local:latest"
    local_image_archive: "/tmp/sae-local-image.tar"
    build_local_image: true

    parallels_vms:
      - { name: "ansible", role: "ansible", cpus: 1, memory_mb: 1024 }
      - { name: "master", role: "master", cpus: 2, memory_mb: 2048 }
      - { name: "worker1", role: "worker", cpus: 1, memory_mb: 1536 }
      - { name: "worker2", role: "worker", cpus: 1, memory_mb: 1536 }

  tasks:
    - name: Ensure Parallels CLI is installed
      ansible.builtin.command: command -v prlctl
      changed_when: false

    - name: Ensure local SSH public key exists
>>>>>>> 8727e93 (local Mac)
      ansible.builtin.shell: |
        if [ -f "$HOME/.ssh/id_ed25519.pub" ]; then
          cat "$HOME/.ssh/id_ed25519.pub"
        elif [ -f "$HOME/.ssh/id_rsa.pub" ]; then
          cat "$HOME/.ssh/id_rsa.pub"
        else
          exit 1
        fi
      register: local_ssh_public_key
      changed_when: false
<<<<<<< HEAD
      failed_when: local_ssh_public_key.stdout | trim == ""

    - name: Detect local private SSH key path
=======

    - name: Ensure local SSH private key exists
>>>>>>> 8727e93 (local Mac)
      ansible.builtin.shell: |
        if [ -f "$HOME/.ssh/id_ed25519" ]; then
          echo "$HOME/.ssh/id_ed25519"
        elif [ -f "$HOME/.ssh/id_rsa" ]; then
          echo "$HOME/.ssh/id_rsa"
        else
          exit 1
        fi
      register: local_ssh_private_key_path
      changed_when: false
<<<<<<< HEAD
      failed_when: local_ssh_private_key_path.stdout | trim == ""

    - name: Clone VMs from template
      community.proxmox.proxmox_kvm:
        api_host: "{{ proxmox_api_host }}"
        api_user: "{{ proxmox_api_user }}"
        api_token_id: "{{ proxmox_api_token_id }}"
        api_token_secret: "{{ proxmox_api_token_secret }}"
        validate_certs: false
        node: "{{ proxmox_node }}"
        clone: "{{ clone_source }}"
        newid: "{{ item.vmid }}"
        name: "{{ item.name }}"
        cores: "{{ item.cores }}"
        memory: "{{ item.memory }}"
        full: true
        timeout: 300
        state: present
      loop: "{{ k3s_cluster_vms }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Configure cloud-init and static IP
      community.proxmox.proxmox_kvm:
        api_host: "{{ proxmox_api_host }}"
        api_user: "{{ proxmox_api_user }}"
        api_token_id: "{{ proxmox_api_token_id }}"
        api_token_secret: "{{ proxmox_api_token_secret }}"
        validate_certs: false
        node: "{{ proxmox_node }}"
        vmid: "{{ item.vmid }}"
        ciuser: "{{ vm_ssh_user }}"
        cipassword: "{{ vm_passwords[item.name] }}"
        citype: nocloud
        sshkeys: "{{ local_ssh_public_key.stdout }}"
        ipconfig:
          ipconfig0: "ip={{ item.ip }}/{{ vm_netmask_cidr }},gw={{ vm_gateway }}"
        onboot: true
        agent: true
        template: false
        update: true
        state: present
      loop: "{{ k3s_cluster_vms }}"
      loop_control:
        label: "{{ item.name }}"
      no_log: true

    - name: Resize VM disk on scsi0
      community.proxmox.proxmox_disk:
        api_host: "{{ proxmox_api_host }}"
        api_user: "{{ proxmox_api_user }}"
        api_token_id: "{{ proxmox_api_token_id }}"
        api_token_secret: "{{ proxmox_api_token_secret }}"
        validate_certs: false
        vmid: "{{ item.vmid }}"
        disk: scsi0
        state: resized
        size: "{{ vm_disk_target_size }}"
      loop: "{{ k3s_cluster_vms }}"
      loop_control:
        label: "{{ item.name }}"
      register: disk_resize_scsi
      failed_when: false

    - name: Resize VM disk on virtio0 fallback
      community.proxmox.proxmox_disk:
        api_host: "{{ proxmox_api_host }}"
        api_user: "{{ proxmox_api_user }}"
        api_token_id: "{{ proxmox_api_token_id }}"
        api_token_secret: "{{ proxmox_api_token_secret }}"
        validate_certs: false
        vmid: "{{ item.vmid }}"
        disk: virtio0
        state: resized
        size: "{{ vm_disk_target_size }}"
      loop: "{{ k3s_cluster_vms }}"
      loop_control:
        label: "{{ item.name }}"
        index_var: vm_idx
      when: disk_resize_scsi is defined and (disk_resize_scsi.results[vm_idx].failed | default(false))
      failed_when: false

    - name: Start VMs
      community.proxmox.proxmox_kvm:
        api_host: "{{ proxmox_api_host }}"
        api_user: "{{ proxmox_api_user }}"
        api_token_id: "{{ proxmox_api_token_id }}"
        api_token_secret: "{{ proxmox_api_token_secret }}"
        validate_certs: false
        node: "{{ proxmox_node }}"
        vmid: "{{ item.vmid }}"
        state: started
        timeout: 300
      loop: "{{ k3s_cluster_vms }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Register provisioned hosts in runtime inventory
      ansible.builtin.add_host:
        name: "{{ item.name }}"
        groups: "provisioned_nodes,{{ 'k3s_master' if item.name == 'master' else 'k3s_workers' }}"
        ansible_host: "{{ item.ip }}"
        ansible_user: "{{ vm_ssh_user }}"
        ansible_ssh_private_key_file: "{{ local_ssh_private_key_path.stdout | trim }}"
        ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
        ansible_python_interpreter: "/usr/bin/python3"
      loop: "{{ k3s_cluster_vms }}"
      loop_control:
        label: "{{ item.name }}"
      no_log: true

- name: Prepare OS and container registry on all K3s nodes
  hosts: provisioned_nodes
  gather_facts: false
  become: true
  vars:
    registry_host: "10.129.4.175:5050"
  tasks:
    - name: Wait until SSH TCP port is open on new VMs
=======

    - name: List available Parallels VM names
      ansible.builtin.command: prlctl list -a -o name
      register: parallels_vm_list
      changed_when: false

    - name: Resolve template VM automatically when not provided
      ansible.builtin.set_fact:
        selected_template_vm: >-
          {{
            (parallels_template_vm | trim)
            if (parallels_template_vm | trim | length > 0)
            else (
              (parallels_vm_list.stdout_lines
                | reject('equalto', 'NAME')
                | select('search', 'Debian.*12')
                | list
                | first
              ) | default('')
            )
          }}

    - name: Ensure a Parallels template/base VM is available
      ansible.builtin.assert:
        that:
          - selected_template_vm | length > 0
        fail_msg: "No base VM found. Set -e parallels_template_vm='<existing_vm_name>' (example: Debian GNU Linux 12.11 ARM64)."

    - name: Stop existing VMs when forced recreate is enabled
      ansible.builtin.command: prlctl stop {{ item.name }} --kill
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"
      when: force_recreate_vms | bool
      failed_when: false
      changed_when: true

    - name: Delete existing VMs when forced recreate is enabled
      ansible.builtin.command: prlctl delete {{ item.name }}
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"
      when: force_recreate_vms | bool
      failed_when: false
      changed_when: true

    - name: Check if VM already exists
      ansible.builtin.command: prlctl list -a --info {{ item.name }}
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"
      register: vm_exists_checks
      failed_when: false
      changed_when: false

    - name: Create linked clones from template when missing
      ansible.builtin.command:
        argv:
          - prlctl
          - clone
          - "{{ selected_template_vm }}"
          - --name
          - "{{ item.item.name }}"
          - --linked
      loop: "{{ vm_exists_checks.results }}"
      loop_control:
        label: "{{ item.item.name }}"
      when: item.rc != 0

    - name: Configure VM CPU and RAM profile
      ansible.builtin.command: prlctl set {{ item.name }} --cpus {{ item.cpus }} --memsize {{ item.memory_mb }}
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Configure VM network mode
      ansible.builtin.command: prlctl set {{ item.name }} --device-set net0 --type {{ parallels_network_mode }}
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Start all VMs
      ansible.builtin.command: prlctl start {{ item.name }}
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"
      failed_when: false
      changed_when: true

    - name: Wait until each VM reports an IP through Parallels Tools
      ansible.builtin.shell: |
        for i in $(seq 1 90); do
          IP=$(prlctl list -a -o name,ip | awk '$1=="{{ item.name }}" {print $2}')
          if [ -n "$IP" ]; then
            echo "$IP"
            exit 0
          fi
          sleep 2
        done
        exit 1
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"
      register: vm_ip_results
      changed_when: false

    - name: Build discovered IP map
      ansible.builtin.set_fact:
        discovered_ips: "{{ discovered_ips | default({}) | combine({item.item.name: (item.stdout | trim)}) }}"
      loop: "{{ vm_ip_results.results }}"
      loop_control:
        label: "{{ item.item.name }}"

    - name: Bootstrap SSH user, sudo and sshd in every VM
      ansible.builtin.shell: |
        prlctl exec {{ item.name }} "bash -lc '
          set -e
          export DEBIAN_FRONTEND=noninteractive
          apt-get update >/dev/null 2>&1 || true
          apt-get install -y sudo openssh-server >/dev/null 2>&1 || true
          id -u {{ vm_ssh_user }} >/dev/null 2>&1 || useradd -m -s /bin/bash {{ vm_ssh_user }}
          echo {{ vm_ssh_user }}:{{ vm_ssh_password }} | chpasswd
          usermod -aG sudo {{ vm_ssh_user }} || true
          mkdir -p /etc/sudoers.d
          echo \"{{ vm_ssh_user }} ALL=(ALL) NOPASSWD:ALL\" > /etc/sudoers.d/90-{{ vm_ssh_user }}
          chmod 440 /etc/sudoers.d/90-{{ vm_ssh_user }}
          systemctl enable ssh >/dev/null 2>&1 || true
          systemctl restart ssh >/dev/null 2>&1 || systemctl restart sshd >/dev/null 2>&1 || true
        '"
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"
      changed_when: true

    - name: Inject local SSH public key inside VMs
      ansible.builtin.shell: |
        prlctl exec {{ item.name }} "bash -lc 'mkdir -p /home/{{ vm_ssh_user }}/.ssh && chmod 700 /home/{{ vm_ssh_user }}/.ssh && echo \"{{ local_ssh_public_key.stdout | trim }}\" >> /home/{{ vm_ssh_user }}/.ssh/authorized_keys && chmod 600 /home/{{ vm_ssh_user }}/.ssh/authorized_keys && chown -R {{ vm_ssh_user }}:{{ vm_ssh_user }} /home/{{ vm_ssh_user }}/.ssh'"
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"
      changed_when: true
      failed_when: false

    - name: Remove stale SSH host keys
      ansible.builtin.command: ssh-keygen -R {{ discovered_ips[item.name] }}
      loop: "{{ parallels_vms }}"
      loop_control:
        label: "{{ item.name }}"
      failed_when: false
      changed_when: false

    - name: Register ansible VM in runtime inventory
      ansible.builtin.add_host:
        name: ansible-vm
        groups: ansible_vm
        ansible_host: "{{ discovered_ips['ansible'] }}"
        ansible_user: "{{ vm_ssh_user }}"
        ansible_ssh_private_key_file: "{{ local_ssh_private_key_path.stdout | trim }}"
        ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ControlMaster=no -o ControlPersist=no"
        ansible_python_interpreter: "/usr/bin/python3"

    - name: Register master and workers in runtime inventory
      ansible.builtin.add_host:
        name: "{{ item.name }}"
        groups: "k3s_nodes,{{ 'k3s_master' if item.role == 'master' else 'k3s_workers' }}"
        ansible_host: "{{ discovered_ips[item.name] }}"
        ansible_user: "{{ vm_ssh_user }}"
        ansible_ssh_private_key_file: "{{ local_ssh_private_key_path.stdout | trim }}"
        ansible_ssh_common_args: "-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ControlMaster=no -o ControlPersist=no"
        ansible_python_interpreter: "/usr/bin/python3"
      loop: "{{ parallels_vms | rejectattr('role', 'equalto', 'ansible') | list }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Show discovered local VM IPs
      ansible.builtin.debug:
        msg:
          ansible_vm: "{{ discovered_ips['ansible'] }}"
          master: "{{ discovered_ips['master'] }}"
          worker1: "{{ discovered_ips['worker1'] }}"
          worker2: "{{ discovered_ips['worker2'] }}"

- name: Prepare OS prerequisites on K3s nodes
  hosts: k3s_nodes
  gather_facts: false
  become: true
  tasks:
    - name: Ensure unique hostname on each K3s node
      ansible.builtin.command: hostnamectl set-hostname {{ inventory_hostname }}
      changed_when: false

    - name: Wait for SSH access
>>>>>>> 8727e93 (local Mac)
      ansible.builtin.wait_for:
        host: "{{ ansible_host }}"
        port: 22
        delay: 2
<<<<<<< HEAD
        timeout: 600
=======
        timeout: 300
>>>>>>> 8727e93 (local Mac)
      delegate_to: localhost
      become: false
      changed_when: false

<<<<<<< HEAD
    - name: Wait until remote commands execute via SSH
      ansible.builtin.raw: echo ready
      register: ssh_ready_raw
      retries: 30
      delay: 5
      until: ssh_ready_raw.rc == 0
      changed_when: false

    - name: Wait for cloud-init to complete
=======
    - name: Wait for cloud-init completion
>>>>>>> 8727e93 (local Mac)
      ansible.builtin.raw: cloud-init status --wait || true
      changed_when: false
      failed_when: false

    - name: Install base packages
      ansible.builtin.raw: |
        export DEBIAN_FRONTEND=noninteractive
        apt-get update
<<<<<<< HEAD
        apt-get install -y curl ca-certificates python3
=======
        apt-get install -y curl ca-certificates python3 rsync
>>>>>>> 8727e93 (local Mac)
      changed_when: false

    - name: Disable swap permanently
      ansible.builtin.raw: |
        swapoff -a || true
        sed -ri '/\sswap\s/s/^#?/#/' /etc/fstab
      changed_when: false

    - name: Load required kernel modules
      ansible.builtin.raw: |
        modprobe overlay || true
        modprobe br_netfilter || true
      changed_when: false

    - name: Configure Kubernetes sysctl settings
      ansible.builtin.copy:
        dest: /etc/sysctl.d/99-kubernetes-cri.conf
        mode: "0644"
        content: |
          net.bridge.bridge-nf-call-iptables = 1
          net.ipv4.ip_forward = 1
          net.bridge.bridge-nf-call-ip6tables = 1

    - name: Apply sysctl settings
      ansible.builtin.command: sysctl --system
      changed_when: false

<<<<<<< HEAD
    - name: Ensure K3s config directory exists
      ansible.builtin.file:
        path: /etc/rancher/k3s
        state: directory
        mode: "0755"

    - name: Configure insecure HTTP registry for K3s runtime
      ansible.builtin.copy:
        dest: /etc/rancher/k3s/registries.yaml
        mode: "0644"
        content: |
          mirrors:
            "{{ registry_host }}":
              endpoint:
                - "http://{{ registry_host }}"

=======
>>>>>>> 8727e93 (local Mac)
- name: Install K3s server on master
  hosts: k3s_master
  gather_facts: false
  become: true
  tasks:
    - name: Install K3s server
      ansible.builtin.shell: |
        curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server --write-kubeconfig-mode 644 --node-ip {{ ansible_host }}" sh -
      args:
        creates: /etc/rancher/k3s/k3s.yaml

<<<<<<< HEAD
    - name: Restart K3s server to reload registries config
      ansible.builtin.service:
        name: k3s
        state: restarted

    - name: Wait for K3s API readyz
=======
    - name: Wait for K3s API readiness
>>>>>>> 8727e93 (local Mac)
      ansible.builtin.command: k3s kubectl get --raw=/readyz
      register: master_ready
      retries: 30
      delay: 5
      until: master_ready.rc == 0
      changed_when: false

    - name: Read K3s node token
      ansible.builtin.command: cat /var/lib/rancher/k3s/server/node-token
      register: k3s_node_token_cmd
      changed_when: false

- name: Install K3s agents on workers
  hosts: k3s_workers
<<<<<<< HEAD
=======
  serial: 1
>>>>>>> 8727e93 (local Mac)
  gather_facts: false
  become: true
  vars:
    k3s_master_host: "{{ hostvars[groups['k3s_master'][0]].ansible_host }}"
    k3s_node_token: "{{ hostvars[groups['k3s_master'][0]].k3s_node_token_cmd.stdout | trim }}"
  tasks:
    - name: Install K3s agent
      ansible.builtin.shell: |
        curl -sfL https://get.k3s.io | K3S_URL="https://{{ k3s_master_host }}:6443" K3S_TOKEN="{{ k3s_node_token }}" INSTALL_K3S_EXEC="agent --node-ip {{ ansible_host }}" sh -
      args:
        creates: /etc/systemd/system/k3s-agent.service

<<<<<<< HEAD
    - name: Restart K3s agent to reload registries config
      ansible.builtin.service:
        name: k3s-agent
        state: restarted

- name: Deploy SAE app and monitoring stack
  hosts: k3s_master
  gather_facts: false
  become: true
  vars_prompt:
    - name: registry_username
      prompt: "GitLab registry username"
      private: false
      default: ""
    - name: registry_password
      prompt: "GitLab registry password/token"
      private: true
      default: ""
  vars:
    registry_host: "10.129.5.159:5050"
    app_namespace: "sae-production"
    repo_src_path: "{{ playbook_dir }}/.."
    repo_dst_path: "/opt/sae-dev6.01"
  tasks:
    - name: Wait for all nodes to be Ready
      ansible.builtin.command: k3s kubectl get nodes --no-headers
      register: nodes_status
      retries: 30
      delay: 5
      until: nodes_status.stdout is search(" Ready ")
      changed_when: false

    - name: Copy repository to master node
      ansible.builtin.copy:
        src: "{{ repo_src_path }}/"
        dest: "{{ repo_dst_path }}/"
        mode: preserve

    - name: Apply namespace manifest
      ansible.builtin.command: k3s kubectl apply -f {{ repo_dst_path }}/kubernetes/namespace.yaml
      changed_when: false

    - name: Create or update registry pull secret when credentials are provided
      ansible.builtin.shell: |
        k3s kubectl create secret docker-registry gitlab-registry-secret \
          --docker-server={{ registry_host }} \
          --docker-username={{ registry_username }} \
          --docker-password={{ registry_password }} \
          -n {{ app_namespace }} \
          --dry-run=client -o yaml | k3s kubectl apply -f -
      changed_when: false
      no_log: true
      when:
        - registry_username | length > 0
        - registry_password | length > 0

    - name: Ensure registry pull secret exists when credentials were not provided
      ansible.builtin.command: k3s kubectl get secret gitlab-registry-secret -n {{ app_namespace }}
      changed_when: false
      when:
        - registry_username | length == 0 or registry_password | length == 0

    - name: Apply SAE app manifests
      ansible.builtin.command: k3s kubectl apply -f {{ repo_dst_path }}/kubernetes/{{ item }}
      loop:
        - deployment.yaml
        - service.yaml
      changed_when: false

    - name: Apply monitoring manifests
      ansible.builtin.command: k3s kubectl apply -f {{ repo_dst_path }}/kubernetes/{{ item }}
      loop:
        - prometheus-config.yaml
        - prometheus.yaml
        - grafana.yaml
      changed_when: false

    - name: Restart deployments to load latest config and images
=======
    - name: Ensure K3s agent service is enabled and running
      ansible.builtin.service:
        name: k3s-agent
        enabled: true
        state: started

- name: Build local Tornado image on macOS host
  hosts: localhost
  gather_facts: false
  vars:
    local_app_image: "sae-local:latest"
    local_image_archive: "/tmp/sae-local-image.tar"
    build_local_image: true
    repo_src_path: "{{ playbook_dir }}/.."
  tasks:
    - name: Ensure Docker CLI exists for local build
      ansible.builtin.command: command -v docker
      changed_when: false
      when: build_local_image | bool

    - name: Check Docker daemon availability
      ansible.builtin.command: docker info
      register: docker_info_check
      changed_when: false
      failed_when: false
      when: build_local_image | bool

    - name: Start Docker Desktop when daemon is unavailable
      ansible.builtin.command: open -a Docker
      when:
        - build_local_image | bool
        - docker_info_check.rc != 0
      changed_when: true
      failed_when: false

    - name: Wait for Docker daemon to become ready
      ansible.builtin.command: docker info
      register: docker_daemon_ready
      retries: 40
      delay: 3
      until: docker_daemon_ready.rc == 0
      changed_when: false
      failed_when: false
      when: build_local_image | bool

    - name: Resolve if local image build is available
      ansible.builtin.set_fact:
        effective_build_local_image: "{{ build_local_image and (docker_daemon_ready.rc | default(1)) == 0 }}"

    - name: Build local app image
      ansible.builtin.command: docker build -t {{ local_app_image }} {{ repo_src_path }}
      when: effective_build_local_image | bool

    - name: Export local app image tar archive
      ansible.builtin.command: docker save -o {{ local_image_archive }} {{ local_app_image }}
      when: effective_build_local_image | bool

    - name: Show build mode
      ansible.builtin.debug:
        msg: "effective_build_local_image={{ effective_build_local_image }}"

- name: Import local app image into all K3s nodes
  hosts: k3s_master
  gather_facts: false
  become: true
  vars:
    local_image_archive: "/tmp/sae-local-image.tar"
    effective_build_local_image: "{{ hostvars['localhost'].effective_build_local_image | default(false) }}"
  tasks:
    - name: Ensure K3s runtime service is running on each node
      ansible.builtin.shell: systemctl restart k3s
      changed_when: false

    - name: Wait for containerd socket on each node
      ansible.builtin.shell: |
        for i in $(seq 1 90); do
          [ -S /run/k3s/containerd/containerd.sock ] && exit 0
          sleep 2
        done
        exit 1
      changed_when: false

    - name: Copy local app image tar to node
      ansible.builtin.copy:
        src: "{{ local_image_archive }}"
        dest: "{{ local_image_archive }}"
        mode: "0644"
      when: effective_build_local_image | bool

    - name: Import image into K3s container runtime
      ansible.builtin.command: k3s ctr images import {{ local_image_archive }}
      when: effective_build_local_image | bool

- name: Deploy Tornado app, Prometheus and Grafana
  hosts: k3s_master
  gather_facts: false
  become: true
  vars:
    app_namespace: "sae-production"
    repo_src_path: "{{ playbook_dir }}/.."
    repo_dst_path: "/opt/sae-dev6.01"
    vm_ssh_user: "debian"
    local_app_image: "sae-local:latest"
    fallback_app_image: "10.129.5.159:5050/root/sae-dev6.01/sae-test:latest"
    effective_build_local_image: "{{ hostvars['localhost'].effective_build_local_image | default(false) }}"
  tasks:
    - name: Resolve desired app image
      ansible.builtin.set_fact:
        desired_app_image: "{{ local_app_image if effective_build_local_image | bool else fallback_app_image }}"

    - name: Wait for all K3s nodes to be Ready
      ansible.builtin.command: k3s kubectl get nodes --no-headers
      register: nodes_status
      retries: 40
      delay: 5
      until: (nodes_status.stdout_lines | select('search', ' Ready ') | list | length) >= 1
      changed_when: false

    - name: Remove obsolete legacy master node object
      ansible.builtin.command: k3s kubectl delete node julien-vm --ignore-not-found=true
      changed_when: false
      failed_when: false

    - name: Ensure repository destination directory is writable by SSH user
      ansible.builtin.file:
        path: "{{ repo_dst_path }}"
        state: absent

    - name: Recreate repository destination directory for SSH user
      ansible.builtin.file:
        path: "{{ repo_dst_path }}"
        state: directory
        owner: "{{ vm_ssh_user }}"
        group: "{{ vm_ssh_user }}"
        mode: "0755"

    - name: Sync repository to master node
      ansible.builtin.shell: >-
        rsync -az --delete --exclude '.git' \
        -e "ssh -i {{ hostvars['localhost'].local_ssh_private_key_path.stdout | trim }} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
        {{ repo_src_path }}/ {{ vm_ssh_user }}@{{ ansible_host }}:{{ repo_dst_path }}/
      delegate_to: localhost
      become: false
      changed_when: true

    - name: Apply namespaces and services
      ansible.builtin.command: k3s kubectl apply -f {{ repo_dst_path }}/kubernetes/{{ item }}
      loop:
        - namespace.yaml
        - service.yaml
        - prometheus-config.yaml
        - prometheus.yaml
        - grafana.yaml
        - deployment.yaml
      changed_when: false

    - name: Set desired image in deployment
      ansible.builtin.command: k3s kubectl -n {{ app_namespace }} set image deployment/sae-app sae-app={{ desired_app_image }}
      changed_when: false

    - name: Pin sae-app to master and scale down for local mode
      ansible.builtin.command: >-
        k3s kubectl -n {{ app_namespace }} patch deployment sae-app --type='merge'
        -p '{"spec":{"replicas":1,"template":{"spec":{"nodeName":"{{ inventory_hostname }}"}}}}'
      changed_when: false

    - name: Force-delete lingering terminating sae-app pods
      ansible.builtin.shell: |
        k3s kubectl -n {{ app_namespace }} get pods --no-headers 2>/dev/null | awk '/^sae-app-.*Terminating/ {print $1}' | \
        xargs -r -I{} k3s kubectl -n {{ app_namespace }} delete pod {} --force --grace-period=0
      changed_when: false
      failed_when: false

    - name: Set imagePullPolicy to IfNotPresent
      ansible.builtin.command: >-
        k3s kubectl -n {{ app_namespace }} patch deployment sae-app --type='strategic'
        -p '{"spec":{"template":{"spec":{"containers":[{"name":"sae-app","imagePullPolicy":"IfNotPresent"}]}}}}'
      when: effective_build_local_image | bool
      changed_when: false

    - name: Remove imagePullSecrets when using local image
      ansible.builtin.command: >-
        k3s kubectl -n {{ app_namespace }} patch deployment sae-app --type='strategic'
        -p '{"spec":{"template":{"spec":{"imagePullSecrets":null}}}}'
      changed_when: false
      failed_when: false

    - name: Restart all deployments
>>>>>>> 8727e93 (local Mac)
      ansible.builtin.shell: |
        k3s kubectl rollout restart deployment/sae-app -n sae-production
        k3s kubectl rollout restart deployment/prometheus -n default
        k3s kubectl rollout restart deployment/grafana -n default
      changed_when: false

<<<<<<< HEAD
    - name: Wait for SAE app rollout
      ansible.builtin.command: k3s kubectl rollout status deployment/sae-app -n sae-production --timeout=300s
      changed_when: false

    - name: Wait for Prometheus rollout
      ansible.builtin.command: k3s kubectl rollout status deployment/prometheus -n default --timeout=300s
      changed_when: false

    - name: Wait for Grafana rollout
      ansible.builtin.command: k3s kubectl rollout status deployment/grafana -n default --timeout=300s
      changed_when: false

    - name: Wait for Tornado NodePort availability
      ansible.builtin.wait_for:
        host: "{{ ansible_host }}"
        port: 30080
        delay: 2
        timeout: 180
      delegate_to: localhost
      become: false

    - name: Seed initial Tornado data when address book is empty
      ansible.builtin.shell: |
        BASE="http://{{ ansible_host }}:30080"
        CURRENT=$(curl -fsS "$BASE/addresses")
        if [ "$CURRENT" = "{}" ]; then
          curl -fsS -X POST "$BASE/addresses" \
            -H "Content-Type: application/json" \
            -d '{"full_name":"Seed User","addresses":[{"kind":"home","street_name":"Main Street","pincode":"75001","country":"France"}],"phone_numbers":[{"kind":"home","country_code":33,"local_number":123456789}],"emails":[{"kind":"home","email":"seed.user@example.com"}]}' >/dev/null
        fi
      args:
        executable: /bin/bash
      changed_when: false
      delegate_to: localhost
      become: false

    - name: Verify Tornado endpoint returns non-empty data
      ansible.builtin.shell: |
        curl -fsS "http://{{ ansible_host }}:30080/addresses" | python3 -c 'import json,sys; data=json.load(sys.stdin); assert isinstance(data,dict) and len(data)>0, "address book is empty"'
      changed_when: false
      delegate_to: localhost
      become: false

    - name: Verify Prometheus NodePort availability
      ansible.builtin.wait_for:
        host: "{{ ansible_host }}"
        port: 30090
        delay: 2
        timeout: 180
      delegate_to: localhost
      become: false

    - name: Verify Prometheus scrapes sae-app successfully
      ansible.builtin.shell: |
        curl -fsS "http://{{ ansible_host }}:30090/api/v1/query?query=up%7Bjob%3D%22sae-app%22%7D" | python3 -c 'import json,sys; payload=json.load(sys.stdin); results=payload["data"]["result"]; assert len(results)>=1, "no sae-app targets"; assert any(float(r["value"][1])>=1.0 for r in results), "sae-app targets are not UP"'
      changed_when: false
      delegate_to: localhost
      become: false

    - name: Wait for Grafana NodePort availability
      ansible.builtin.wait_for:
        host: "{{ ansible_host }}"
        port: 30030
        delay: 2
        timeout: 180
      delegate_to: localhost
      become: false

    - name: Create base Grafana dashboard JSON
      ansible.builtin.copy:
        dest: /tmp/sae-base-dashboard.json
        mode: "0644"
        content: |
          {
            "dashboard": {
              "id": null,
              "uid": "sae-base",
              "title": "SAE Base Monitoring",
              "tags": ["sae", "k3s"],
              "timezone": "browser",
              "schemaVersion": 39,
              "version": 1,
              "refresh": "5s",
              "time": {"from": "now-15m", "to": "now"},
              "panels": [
                {
                  "id": 1,
                  "type": "timeseries",
                  "title": "Pods UP (sae-app)",
                  "gridPos": {"h": 8, "w": 24, "x": 0, "y": 0},
                  "targets": [{"refId": "A", "expr": "up{job=\"sae-app\"}", "legendFormat": "{{instance}}"}]
                },
                {
                  "id": 2,
                  "type": "timeseries",
                  "title": "HTTP Requests/sec",
                  "gridPos": {"h": 8, "w": 24, "x": 0, "y": 8},
                  "targets": [{"refId": "A", "expr": "sum by (method, endpoint, status) (rate(addrservice_http_requests_total[1m]))", "legendFormat": "{{method}} {{endpoint}} {{status}}"}]
                },
                {
                  "id": 3,
                  "type": "timeseries",
                  "title": "P95 Request Duration",
                  "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16},
                  "targets": [{"refId": "A", "expr": "histogram_quantile(0.95, sum by (le, endpoint) (rate(addrservice_http_request_duration_seconds_bucket[5m])))", "legendFormat": "{{endpoint}}"}]
                }
              ]
            },
            "folderId": 0,
            "overwrite": true
          }

    - name: Import base Grafana dashboard
      ansible.builtin.shell: |
        curl -fsS -u admin:admin \
          -H "Content-Type: application/json" \
          -X POST "http://{{ ansible_host }}:30030/api/dashboards/db" \
          -d @/tmp/sae-base-dashboard.json >/tmp/grafana_import.out
      args:
        executable: /bin/bash
      changed_when: false

    - name: Verify dashboard exists in Grafana
      ansible.builtin.shell: |
        curl -fsS -u admin:admin "http://{{ ansible_host }}:30030/api/search?query=SAE%20Base%20Monitoring" | python3 -c 'import json,sys; data=json.load(sys.stdin); assert any(d.get("title")=="SAE Base Monitoring" for d in data), "dashboard not found"'
      changed_when: false
      delegate_to: localhost
      become: false

=======
    - name: Wait for sae-app rollout
      ansible.builtin.command: k3s kubectl rollout status deployment/sae-app -n sae-production --watch=false
      register: sae_app_rollout
      retries: 84
      delay: 5
      until: sae_app_rollout.rc == 0
      changed_when: false

    - name: Wait for Prometheus rollout
      ansible.builtin.command: k3s kubectl rollout status deployment/prometheus -n default --watch=false
      register: prometheus_rollout
      retries: 84
      delay: 5
      until: prometheus_rollout.rc == 0
      changed_when: false

    - name: Wait for Grafana rollout
      ansible.builtin.command: k3s kubectl rollout status deployment/grafana -n default --watch=false
      register: grafana_rollout
      retries: 84
      delay: 5
      until: grafana_rollout.rc == 0
      changed_when: false

>>>>>>> 8727e93 (local Mac)
    - name: Show deployment summary
      ansible.builtin.shell: |
        echo "=== Nodes ==="
        k3s kubectl get nodes -o wide
<<<<<<< HEAD
        echo "=== sae-app Pods ==="
        k3s kubectl get pods -n sae-production -o wide
        echo "=== Prometheus/Grafana Pods ==="
        k3s kubectl get pods -n default -l app in \(prometheus,grafana\) -o wide
        echo "=== Services ==="
        k3s kubectl get svc -A
        echo "=== Access URLs ==="
        echo "Tornado: http://{{ ansible_host }}:30080/addresses"
        echo "Prometheus: http://{{ ansible_host }}:30090"
        echo "Grafana: http://{{ ansible_host }}:30030 (admin/admin)"
        echo "=== VM credentials ==="
        echo "master -> user: {{ vm_ssh_user }} / password: {{ vm_passwords['master'] }}"
        echo "worker1 -> user: {{ vm_ssh_user }} / password: {{ vm_passwords['worker1'] }}"
        echo "worker2 -> user: {{ vm_ssh_user }} / password: {{ vm_passwords['worker2'] }}"
      args:
        executable: /bin/bash
      changed_when: false
=======
        echo "=== sae-app pods ==="
        k3s kubectl get pods -n sae-production -o wide
        echo "=== monitoring pods ==="
        k3s kubectl get pods -n default -l app in \(prometheus,grafana\) -o wide
        echo "=== Services ==="
        k3s kubectl get svc -A
      changed_when: false

- name: Validate service accessibility from localhost via SSH
  hosts: localhost
  gather_facts: false
  vars:
    master_host: "{{ hostvars[groups['k3s_master'][0]].ansible_host }}"
  tasks:
    - name: Display final local URLs
      ansible.builtin.debug:
        msg:
          - "Tornado: http://{{ master_host }}:30080/addresses"
          - "Prometheus: http://{{ master_host }}:30090"
          - "Grafana: http://{{ master_host }}:30030 (admin/admin)"
          - "Ansible VM: {{ hostvars['ansible-vm'].ansible_host }}"
>>>>>>> 8727e93 (local Mac)
